---
title: "Biostat 203B Homework 4"
subtitle: Due Mar 24 @ 11:59PM
author: Saanchi Shah and 204591578
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
knitr:
  opts_chunk: 
    cache: false    
    echo: true
    fig.align: 'center'
    fig.width: 6
    fig.height: 4
    message: FALSE
---

Display machine information:

```{r}
#| eval: false

sessionInfo()
```

Load database libraries and the tidyverse frontend:

```{r}
#| eval: false

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(GGally))
suppressPackageStartupMessages(library(gtsummary))
# install.packages("xgboost")
suppressPackageStartupMessages(library(xgboost))
suppressPackageStartupMessages(library(parsnip))
suppressPackageStartupMessages(library(parsnip))

```

## Predicting 30-day mortality

Using the ICU cohort `icu_cohort.rds` you built in Homework 3, develop at least three analytic approaches for predicting the 30-day mortality of patients admitted to ICU using demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, and first vital measurements during ICU stay. For example, you can use (1) logistic regression with elastic net (lasso + ridge) penalty (e.g., glmnet or keras package), (2) random forest, (3) boosting, and (4) support vector machines, or (5) MLP neural network (keras package)

First, I will modify my ICU.rds file because I had an erroneous variable in my original file.

```{r}
library(tidyverse)

icu_cohort <- readRDS(file = file.path("/Users/saanchishah/biostat-203b-2023-winter/hw3/mimiciv_shiny", 'icu_cohort.rds'))

icu_cohort_new <- icu_cohort %>% 
  mutate(duration_new = difftime(dod, admittime, unit = 'days'),
         thirty_day_mort_new = if_else(duration_new < 30, 1, 0)) %>% 
  select(-c(duration, thirty_day_mort))

icu_cohort_new$thirty_day_mort_new <- as.factor(icu_cohort_new$thirty_day_mort_new)

icu_cohort_subset <- 
  icu_cohort_new %>% 
  filter(!is.na(thirty_day_mort_new)) %>% 
  select(c(thirty_day_mort_new,
           gender, 
           age, 
           Potassium, 
           Chloride, 
           Sodium,  
           ethnicity,
           insurance,    
           WBC,
           Hematocrit,
           Sodium,
           Glucose,
           Bicarbonate,
           Creatinine,
           Resp_rate, 
           Diastolic_BP,
           Systolic_BP,
           Temperature, 
           Heart_rate))
  
```

# Q1. Partition data into 50% training set and 50% test set. Stratify partitioning according the 30-day mortality status.

```{r}
library(tidymodels)

# For reproducibility
set.seed(203)

data_split <- initial_split(
  icu_cohort_subset, 
  # stratify by thirty_day_mort
  strata = "thirty_day_mort_new", 
  prop = 0.50 # 50-50 split as suggested in class
  )
data_split

icu_other <- training(data_split) 
# check dimensions of training set
dim(icu_other)


# check dimensions of the training data set
icu_test <- training(data_split)
dim(data_split)


```

# Q2. Train and tune the models using the training set.

### Pre-processing steps
First, I will preprocess the data for logistic regression, xgboost and random forest

```{r}


logit_recipe <- 
  recipe(
    thirty_day_mort_new ~ ., 
    data = icu_other
  ) %>%

  # create traditional dummy variables
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  # step_impute_mean(Potassium, Sodium, Chloride) %>% 
  step_impute_mean(all_numeric_predictors()) %>% # would not work without imputations for me
  step_zv(all_numeric_predictors()) %>% 
  # center and scale numeric data
  step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = icu_other, retain = TRUE)
logit_recipe

# Create model
logit_mod <- 
logistic_reg(
  penalty = tune(),
  mixture = tune()
 ) %>% 
set_engine("glmnet", standardize = FALSE)
logit_mod


# add recipe
logit_wf <- workflow() %>%
  add_recipe(logit_recipe) %>%
  add_model(logit_mod)
logit_wf

# Tune models.

param_grid <- grid_regular(
  penalty(range = c(-6, 3)), 
  mixture(),
  levels = c(100, 5)
  )
param_grid

```

# Q3)A Compare model classification performance on the test set. Report both the area under ROC curve and accuracy for logistic regression.


###Cross-validation for logistic regression + model fitting**

```{r}

# install.packages("rsample")
library(rsample)
library(dplyr)
# install.packages("tune")
library(tune)
library(parsnip)


set.seed(203)
folds <- vfold_cv(icu_other, v = 5)

system.time({
  logit_fit <- logit_wf %>%
    tune_grid(
      resamples = folds,
      grid = param_grid,
      metrics = metric_set(roc_auc, accuracy)
    )
})

logit_fit


logit_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = penalty, y = mean, color = mixture)) +
  geom_point() +
  labs(x = "Penalty", y = "CV AUC") +
  scale_x_log10()

logit_fit %>%
  show_best("roc_auc")


best_logit <- logit_fit %>%
  select_best("roc_auc")
best_logit


final_wf <- logit_wf %>%
  finalize_workflow(best_logit)
final_wf


final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit


final_fit %>% 
  collect_metrics()
```

# Q3)B. Random forest.

### 1. Pre-processing steps (split + recipe)
Since the initial split and preprocessing steps are the same, I will leave them untouched and start with the modeling step.

```{r}
# model

rf_mod <- 
  rand_forest(
    mode = "classification",
    # Number of predictors randomly sampled in each split
    mtry = tune(),
    # Number of trees in ensemble
    trees = tune()
  ) %>% 
  set_engine("ranger")
rf_mod

# Workflow

rf_wf <- workflow() %>%
  add_recipe(logit_recipe) %>% # including logit_recipe because the recipe is =
  add_model(rf_mod)
rf_wf


# tuning grid

param_grid <- grid_regular(
  trees(range = c(100L, 300L)), 
  mtry(range = c(1L, 5L)),
  levels = c(3, 5)
  )
param_grid

# cross validation

set.seed(203)

folds <- vfold_cv(icu_other, v = 5)
folds
```

### 2. Best fit model for random forest

```{r}
# fit cross validation


rf_fit <- rf_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
rf_fit

# visualize fit

rf_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  geom_point() + 
  # geom_line() + 
  labs(x = "Num. of Trees", y = "CV AUC")

# show the 5 best models

rf_fit %>%
  show_best("roc_auc")

# Pick the top model

best_rf <- rf_fit %>%
  select_best("roc_auc")
best_rf

# final workflow

final_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_wf


# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

# Test metrics
final_fit %>% 
  collect_metrics()


```

# Q3)C. Xgboost
### 1. Prepreocessing (split + recipe)
Again, the pre processing steps remain the same so I will start by building the model and go on from there onwards to select the best fit.

```{r}

# build model

gb_mod <- 
  boost_tree(
    mode = "classification",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")
gb_mod

# workflow 

gb_wf <- workflow() %>%
  add_recipe(logit_recipe) %>%
  add_model(gb_mod)
gb_wf


# tuning grid

param_grid <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 10)
  )
param_grid


# Cross validation steps

set.seed(203)

folds <- vfold_cv(icu_other, v = 5)
folds

# fit cross validation

gb_fit <- gb_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
gb_fit


# visualize cross validation

gb_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = learn_rate, y = mean, color = tree_depth)) +
  geom_point() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()

# show top 5 models
gb_fit %>%
  show_best("roc_auc")

# select the best fit model

best_gb <- gb_fit %>%
  select_best("roc_auc")
best_gb
```

### 2. Finalize xgboost model

```{r}
# Final workflow
final_wf <- gb_wf %>%
  finalize_workflow(best_gb)
final_wf


# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit


# Test metrics
final_fit %>% 
  collect_metrics()
```

# Report
**It appears that the accuracy for each of these models is pretty similar.**

Xgboost: ROC = 0.717, Accuracy = 0.748 Random forest: ROC = 0.724, Accuracy = 0.742 Logistic Regression: ROC = 0.70, Accuracy = 0.7449
